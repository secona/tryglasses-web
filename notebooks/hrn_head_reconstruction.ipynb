{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXhE73e1ixo"
      },
      "source": [
        "# HRN Inference\n",
        "\n",
        "This notebook performs HRN inference for the [TryGlasses](https://github.com/secona/tryglasses-web) project.\n",
        "\n",
        "### Acknowledgements\n",
        "The HRN model used in this notebook is kindly provided by [ModelScope](https://modelscope.cn/models/iic/cv_HRN_head-reconstruction/summary).\n",
        "\n",
        "Original Project: [HRN (Hierarchical Representation Network)](https://github.com/youngLBW/HRN)\n",
        "\n",
        "### Estimated Runtime\n",
        "\n",
        "The authors report that the full notebook requires approximately one hour to complete when using the free tier of Google Colab GPU. Most of the time goes into installing dependencies, namely `pytorch3d` and `nvdiffrast`.\n",
        "\n",
        "### Usage Instructions\n",
        "1. **Prepare Input**: Save or upload your source image to the working directory and rename it to `face.jpg`.\n",
        "2. **Execution**: Run the cells below sequentially to generate the inference results.\n",
        "3. **Downloading**: If run on Google Colab, it should automatically download the resulting ZIP file ready for the web app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wnICtlLoIOB"
      },
      "outputs": [],
      "source": [
        "INPUT_IMG_PATH = \"face.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG2wQozh2UPR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(INPUT_IMG_PATH):\n",
        "    raise FileNotFoundError(f\"Error: '{INPUT_IMG_PATH}' was not found in the current directory. Please upload the image and try again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVC_JSdK8gKx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_input_img():\n",
        "  img = cv2.imread(INPUT_IMG_PATH)\n",
        "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  plt.imshow(img_rgb)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "display_input_img()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tqrjfClgxCE"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHwjSe5pIOZ"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTu8HLmRNzwn"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow\n",
        "!pip install \"modelscope[cv]\" -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n",
        "!pip install git+https://github.com/NVlabs/nvdiffrast.git --no-build-isolation\n",
        "!pip install git+https://github.com/facebookresearch/pytorch3d.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-4XccVlMgel"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "from pytorch3d.structures import Meshes\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "from modelscope.models.cv.face_reconstruction.utils import write_obj\n",
        "from modelscope.outputs import OutputKeys\n",
        "from modelscope.pipelines import pipeline\n",
        "from modelscope.utils.constant import Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWKKGTp3hdXS"
      },
      "source": [
        "# Model initialization\n",
        "Initialize the pipeline and download the pretrained weights from modelscope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr1N_7QlPMRs"
      },
      "outputs": [],
      "source": [
        "if not hasattr(torch, '_original_load_unpatched'):\n",
        "    torch._original_load_unpatched = torch.load\n",
        "\n",
        "def strict_bypass_load(*args, **kwargs):\n",
        "    if 'weights_only' not in kwargs:\n",
        "        kwargs['weights_only'] = False\n",
        "\n",
        "    return torch._original_load_unpatched(*args, **kwargs)\n",
        "\n",
        "torch.load = strict_bypass_load\n",
        "\n",
        "head_reconstruction = pipeline(Tasks.head_reconstruction, model='iic/cv_HRN_head-reconstruction', model_revision='v0.1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-ci36Biham"
      },
      "source": [
        "# Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSWrBNL8fBET"
      },
      "outputs": [],
      "source": [
        "result = head_reconstruction(INPUT_IMG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g89JaC8rVrxf"
      },
      "outputs": [],
      "source": [
        "model = head_reconstruction.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2vTRxDkExi"
      },
      "source": [
        "# Extract Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CrzmSbUiX6Q"
      },
      "outputs": [],
      "source": [
        "coeffs = model.pred_coeffs_dict\n",
        "coeffs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqfW8LWHV4Px"
      },
      "outputs": [],
      "source": [
        "rotation = coeffs['angle'].detach().cpu().numpy()\n",
        "position = coeffs['trans'].detach().cpu().numpy()\n",
        "\n",
        "print(\"Rotation (Rad):\", rotation)\n",
        "print(\"Position:\", position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wd-FqpTjfMk"
      },
      "outputs": [],
      "source": [
        "opt = model.opt\n",
        "\n",
        "print(\"--- Camera Intrinsics ---\")\n",
        "\n",
        "print(f\"Focal Length: {opt.focal}\")\n",
        "print(f\"Center (cx, cy): {opt.center}\")\n",
        "\n",
        "print(\"\\n--- Camera Extrinsics & Clipping ---\")\n",
        "\n",
        "print(f\"Camera Distance: {opt.camera_d}\")\n",
        "print(f\"Z Near: {opt.z_near}\")\n",
        "print(f\"Z Far:  {opt.z_far}\")\n",
        "\n",
        "fov = 2 * np.arctan(opt.center / model.opt.focal) * 180 / np.pi\n",
        "print(f\"Calculated FOV: {fov:.2f} degrees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYuN4UpiRhX3"
      },
      "outputs": [],
      "source": [
        "def extract_preprocessed_input_img(model):\n",
        "    input_tensor = model.input_img\n",
        "    input_img_numpy = input_tensor.detach().cpu().numpy()\n",
        "\n",
        "    if input_img_numpy.ndim == 4:\n",
        "        input_img_numpy = input_img_numpy[0]\n",
        "\n",
        "    if input_img_numpy.shape[0] == 3:\n",
        "        input_img_numpy = np.transpose(input_img_numpy, (1, 2, 0))\n",
        "\n",
        "    if input_img_numpy.max() <= 1.0:\n",
        "        input_img_numpy = (input_img_numpy * 255).astype(np.uint8)\n",
        "    else:\n",
        "        input_img_numpy = input_img_numpy.astype(np.uint8)\n",
        "\n",
        "    input_img_rgb = cv2.cvtColor(input_img_numpy, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return input_img_rgb\n",
        "\n",
        "def view_cropped_input_img(model):\n",
        "  input_img_rgb = extract_preprocessed_input_img(model)\n",
        "  original_img_rgb = cv2.cvtColor(cv2.imread(INPUT_IMG_PATH), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "  ax = ax.flatten()\n",
        "\n",
        "  ax[0].imshow(input_img_rgb)\n",
        "  ax[0].axis('off')\n",
        "  ax[0].set_title(\"Preprocessed\")\n",
        "\n",
        "  ax[1].imshow(original_img_rgb)\n",
        "  ax[1].axis('off')\n",
        "  ax[1].set_title(\"Original\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "view_cropped_input_img(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tz3F400V5JH"
      },
      "outputs": [],
      "source": [
        "def verify_projection(model, position, opt):\n",
        "    img = extract_preprocessed_input_img(model)\n",
        "\n",
        "    position = position.flatten()\n",
        "    focal = opt.focal or 1015.0\n",
        "    center = opt.center or 112.0\n",
        "\n",
        "    x = (position[0] / position[2]) * focal + center\n",
        "    y = (position[1] / position[2]) * focal + center\n",
        "\n",
        "    cv2.circle(img, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Projected Center\\n(X={x:.1f}, Y={y:.1f})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "verify_projection(model, position, opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErkK1gx-pI4"
      },
      "source": [
        "# Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJqVIvoXkEli"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_results(result, model, save_root):\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "\n",
        "    mesh = result[OutputKeys.OUTPUT]['mesh']\n",
        "    write_obj(os.path.join(save_root, 'HRN_result.obj'), mesh)\n",
        "\n",
        "    img = extract_preprocessed_input_img(model)\n",
        "    cv2.imwrite(os.path.join(save_root, 'HRN_preprocessed.jpg'), img)\n",
        "\n",
        "    print(f'Output written to {os.path.abspath(save_root)}')\n",
        "\n",
        "save_results(result, model, './HRN_export')\n",
        "!zip -r HRN_export.zip HRN_export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yb1TAgjk2kk"
      },
      "outputs": [],
      "source": [
        "def save_reconstruction_data(filename, model, result, coeffs, opt, image_path):\n",
        "    vertices = model.pred_vertex.detach().cpu().numpy()[0] # [N, 3]\n",
        "    faces = model.headmodel.face_buf.detach().cpu().numpy() # [M, 3]\n",
        "    mesh = result['output']['mesh']\n",
        "\n",
        "    texture_map = result['output_img']\n",
        "    if hasattr(texture_map, 'detach'):\n",
        "        texture_map = texture_map.detach().cpu().numpy()\n",
        "\n",
        "    trans = coeffs['trans'].detach().cpu().numpy().flatten()\n",
        "    angle = coeffs['angle'].detach().cpu().numpy().flatten()\n",
        "\n",
        "    camera_info = {\n",
        "        'focal': opt.focal,\n",
        "        'center': opt.center,\n",
        "        'camera_d': opt.camera_d,\n",
        "        'z_near': opt.z_near,\n",
        "        'z_far': opt.z_far\n",
        "    }\n",
        "\n",
        "    np.savez_compressed(\n",
        "        filename,\n",
        "        vertices=vertices,\n",
        "        faces=faces,\n",
        "        trans=trans,\n",
        "        angle=angle,\n",
        "        camera_focal=camera_info['focal'],\n",
        "        camera_center=camera_info['center'],\n",
        "        camera_dist=camera_info['camera_d'],\n",
        "    )\n",
        "\n",
        "    print(f\"Saved full scene to {filename}\")\n",
        "\n",
        "save_reconstruction_data(\"HRN_recon.npz\", model, result, coeffs, opt, INPUT_IMG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpxOFvuTpLwk"
      },
      "outputs": [],
      "source": [
        "def inspect_saved_file(filename):\n",
        "    data = np.load(filename)\n",
        "\n",
        "    print(\"--- File Contents ---\")\n",
        "    print(f\"Vertices: {data['vertices'].shape}\")\n",
        "    print(f\"Faces:    {data['faces'].shape}\")\n",
        "    print(f\"Position: {data['trans']}\")\n",
        "    print(f\"Rotation: {data['angle']}\")\n",
        "    print(f\"C_Focal:  {data['camera_focal']}\")\n",
        "    print(f\"C_Center: {data['camera_center']}\")\n",
        "    print(f\"C_Dist:   {data['camera_dist']}\")\n",
        "\n",
        "inspect_saved_file(\"HRN_recon.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viMNWX4pvEgx"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "output_filename = f\"TryGlasses_{timestamp}.zip\"\n",
        "\n",
        "!zip {output_filename} \\\n",
        "  HRN_export/HRN_result.obj \\\n",
        "  HRN_export/HRN_result.jpg \\\n",
        "  HRN_export/HRN_preprocessed.jpg \\\n",
        "  HRN_recon.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqHtvTQIvLmB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(output_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}